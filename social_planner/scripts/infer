#!/usr/bin/env python

from __future__ import absolute_import, division, print_function

import argparse
import json
import torch

import lstm_motion_model.utils
from lstm_motion_model import models, datasets


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--weights',
                        help='Load model weights from pickle file')
    parser.add_argument('-c', '--config',
                        help='Config json file use to instantiate the model',
                        default='config.json')
    parser.add_argument('-i', '--input', nargs='+',
                        help='Run inference on csv dataset(s) provided',
                        required=True)
    parser.add_argument('-s', '--sample-steps',
                        help='Number of steps to recursively sample',
                        default=0, type=int)
    parser.add_argument('-m', '--mode', type=str, default=None,
                        help='Plot data with model in given mode')
    parser.add_argument('-o', '--output', help='Save predictions to pkl file',
                        default='results.pkl')
    parser.add_argument('-t', '--stochastic',
                        help='When forecasting with a probabilistic model, '
                        'sample the distribution',
                        action='store_true')
    parser.add_argument('-a', '--augment',
                        help='Apply random augentation to test data',
                        action='store_true')
    parser.add_argument('--no-cuda', action='store_true', help='Disable CUDA')
    parser.add_argument('--device', type=int, default=0, help='Choose device')
    args = parser.parse_args()
    # TODO Fix device management to pick by integer
    use_cuda = (not args.no_cuda) and torch.cuda.is_available()
    device = torch.device('cuda:{}'.format(args.device) if use_cuda else 'cpu')

    # Load model config from file and use it to create the model
    print('Loading model config from {}...'.format(args.config))
    with open(args.config, 'r') as f:
        config = json.load(f)
        print('Creating model from config...')
        model = models.instantiate_model(config['model_type'],
                                         config['model_args'], device)

    # Load model weights from file
    if args.weights:
        print('Loading model weights from {}...'.format(args.weights))
        model.load_state_dict(torch.load(args.weights, map_location=device))
    else:
        print('### WARNING ###')
        print('No weights loaded, fine for a simple model but not for a '
              'learned model')
    model.eval()

    # Compose dataset transform functions
    def data_transform(seq):
        return lstm_motion_model.utils.random_rotate_tensor(
            lstm_motion_model.utils.random_flip_tensor(seq))

    if args.augment:
        tf = data_transform
    else:
        tf = None

    print('Loading input trajectories from', args.input)
    dataset = datasets.CrowdSequenceDataset(
        args.input,
        device=device,
        transform=tf)

    if args.mode:
        try:
            model.mode(args.mode)
            print('Set mode to {}'.format(args.mode))
        except AttributeError:
            print('This model does not support mode changes')

    print('Computing predictions...')
    predictions = compute_social_predictions(
        model, dataset, args.sample_steps, device, stochastic=args.stochastic)

    # Save predictions to file
    with open(args.output, 'wb+') as f:
        torch.save((predictions, config), f)


@torch.no_grad()
def compute_social_predictions(model, dataset, sample_steps, device,
                               stochastic=False):
    model.eval()
    assert sample_steps >= 0

    # Create list to store output for each sequence
    results = []
    # Iterate through the sequences
    for pos_seq in dataset:
        #print(pos_seq[0])
        #print(pos_seq[1])
        #print(pos_seq[2])
        #print(pos_seq.shape)
        # Extract dims of the position sequence for readability
        seq_len, num_people, dims = pos_seq.shape
        # Create an empty dict to store results of this sequence
        results.append({})
        # Create a hidden state per person if applicable
        try:
            hidden = model.init_hidden(num_people, device)
        except AttributeError as e:
            print(e)
            print('init_hidden not implemented. Assuming non-recursive model')
            hidden = None

        # Store input sequence
        results[-1]['true_position'] =\
            pos_seq[model.before_length:len(pos_seq)-model.after_length]

        # Create empty dict to accumulate outputs over the sequence
        outputs_this_sequence = {}

        output_length = seq_len - model.before_length - model.after_length
        valid_frames = torch.zeros((output_length,), dtype=bool,
                                   device=pos_seq.device)
        output_frame = 0
        print(output_length)
        print(seq_len)
        print(valid_frames)

        # Loop through each frame of the sequence (skip first frame)
        for i in range(model.before_length, seq_len-model.after_length):

            # Valid people are those observed in this frame and the one before
            valid_persons = torch.isfinite(
                pos_seq[i - model.before_length:i + model.after_length + 1]
            ).all(-1).all(0)

            if valid_persons.sum() > 0:
                valid_frames[output_frame] = True

                model_input = model.prepare_input(pos_seq[:, valid_persons, :],
                                                  start=i, stop=i+1)
                model.normalise_input(model_input)

                if hidden is None:
                    model_output = model(model_input)
                else:
                    # Extract the valid slices of 'hidden'
                    valid_hidden = tuple(h[:, valid_persons, :] for h in hidden)
                    model_output, valid_hidden = model(
                        model_input, valid_hidden)
                    # Update the valid slices of 'hidden'
                    for h, v in zip(hidden, valid_hidden):
                        h[:, valid_persons, :] = v

                model_output.update(model.sample_output(
                    model_output, stochastic))
                model.unnormalise_target(model_output)
                model_input = model.propagate_input(model_input, model_output)
                model_output.update(model_input)

                # Start a list per item in model_output
                outputs_this_step = {}
                for k, v in model_output.items():
                    outputs_this_step[k] = [v]

                for j in range(sample_steps):
                    if hidden is None:
                        model_output = model(model_input)
                    else:
                        model_output, valid_hidden = model(
                            model_input, valid_hidden)

                    model_output.update(model.sample_output(
                        model_output, stochastic))
                    model_input = model.propagate_input(
                        model_input, model_output)
                    model_output.update(model_input)

                    # Add to the list for each item
                    for k, v in model_output.items():
                        # Dims of each element are (1, person, feature)
                        outputs_this_step[k].append(v)

                for k, v in outputs_this_step.items():
                    # Dims of one step (1, person_valid, pred_frame, feature)
                    one_step = torch.stack(v, dim=2)
                    # Dims of one_step_full (1, person_all, pred_frame, feature)
                    one_step_full = one_step.new_full(
                        (1, num_people) + one_step.shape[2:],
                        float('nan'))
                    one_step_full[0, valid_persons, ...] = one_step
                    if k in outputs_this_sequence:
                        outputs_this_sequence[k].append(one_step_full)
                    else:
                        outputs_this_sequence[k] = [one_step_full]

            output_frame += 1

        for k, v in outputs_this_sequence.items():
            # Dims of seq_output items are (frame, person, pred_frame, feature)
            seq_output = torch.cat(v, dim=0)
            seq_output_full = seq_output.new_full(
                (output_length,) + seq_output.shape[1:], float('nan'))
            seq_output_full[valid_frames, ...] = seq_output
            results[-1][k] = seq_output_full

    return results


if __name__ == '__main__':
    main()
