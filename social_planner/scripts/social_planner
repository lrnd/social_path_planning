#!/usr/bin/env python

import rospy
import argparse
import json
import torch
import time
import math
import random
import time

import lstm_motion_model.utils
from numpy import empty
from lstm_motion_model.robot import Robot
from lstm_motion_model import models, datasets, robot_plots
from pedsim_srvs.srv import GetAgentPos
from geometry_msgs.msg import *
from std_srvs.srv import Empty 

global num_agents 
global num_paths 
global min_seq_req
global rbt_steps 
global p_weight
global d_weight
global max_work_per_ped

num_agents = 29 
num_paths =3
min_seq_req = 3
rbt_steps = 15 
p_weight = 1
d_weight = 2
max_work_per_ped = 3

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--weights',
                        help='Load model weights from pickle file')
    parser.add_argument('-c', '--config',
                        help='Config json file use to instantiate the model',
                        default='config.json')
   # parser.add_argument('-i', '--input', nargs='+',
   #                     help='Run inference on csv dataset(s) provided',
   #                     required=True)
    parser.add_argument('-s', '--sample-steps',
                        help='Number of steps to recursively sample',
                        default=0, type=int)
    parser.add_argument('-m', '--mode', type=str, default=None,
                        help='Plot data with model in given mode')
    parser.add_argument('-o', '--output', help='Save predictions to pkl file',
                        default='results.pkl')
    parser.add_argument('-t', '--stochastic',
                        help='When forecasting with a probabilistic model, '
                        'sample the distribution',
                        action='store_true')
    parser.add_argument('-a', '--augment',
                        help='Apply random augentation to test data',
                        action='store_true')
    parser.add_argument('--no-cuda', action='store_true', help='Disable CUDA')
    parser.add_argument('--device', type=int, default=0, help='Choose device')
    parser.add_argument('-p', '--plot', 
                        help='Plot scenarious with the different paths',
                        default=False, type=bool)

    args = parser.parse_args()

    #initialise Ros node
    rospy.init_node("inference")
    rospy.loginfo("inference running")
    pause_sim(False)

    # TODO Fix device management to pick by integer
    use_cuda = (not args.no_cuda) and torch.cuda.is_available()
    device = torch.device('cuda:{}'.format(args.device) if use_cuda else 'cpu')

    # Load model config from file and use it to create the model
    #print('Loading model config from {}...'.format(args.config))
    with open(args.config, 'r') as f:
        config = json.load(f)
        #print('Creating model from config...')
        model = models.instantiate_model(config['model_type'],
                                         config['model_args'], device)

    # Load model weights from file
    if args.weights:
        #print('Loading model weights from {}...'.format(args.weights))
        model.load_state_dict(torch.load(args.weights, map_location=device))
    else:
        print('### WARNING ###')
        #print('No weights loaded, fine for a simple model but not for a '
              #'learned model')
    model.eval()

    if args.mode:
        try:
            model.mode(args.mode)
            print('Set mode to {}'.format(args.mode))
        except AttributeError:
            print('This model does not support mode changes')

    ##need to pass in the time sequence to generate path for
    #create robot with its pos and goal and speed
    invalid = True 
    while invalid:
        x = float(input("enter robot x position: "))
        y = float(input("enter robot y position: "))
        robot_pos = [x, y]
        x = float(input("enter robot goal x position: "))
        y = float(input("enter robot goal y position: "))
        robot_goal_pos = [x,y]
        if abs(robot_pos[0]) < 15 and abs(robot_goal_pos[0]) < 15 and \
            abs(robot_pos[1]) < 15 and abs(robot_goal_pos[1]) < 15:
            invalid = False
        else:
            print("invalid input, must be: -15 < val < 15")

    #creating robot
    rbt = Robot(robot_pos, robot_goal_pos, ped_buf=0.3)

    print('robot pos: ', robot_pos)
    print('robot goal: ', robot_goal_pos)

    #get data from sim to create dataset just ped ground truths
    dataset = torch.zeros((min_seq_req, num_agents, 2), dtype=torch.float,
    device=device)
    #get data from sim to create dataset with robot
    dataset_rbt = torch.zeros((rbt_steps, num_agents+1, 2), dtype=torch.float,
    device=device)

    ids = []
    pos = []
    for i in range(num_agents):
        ids.append(i)

#start timing:
    time_start = time.process_time()
    #generates the min number of recorded position for APG
    for i in range(min_seq_req):
        pos.append(get_ped_positions(ids))
        time.sleep(0.1)

    pause_sim(True)
    #add pedestrian data into dataset, atm just one set
    for i in range(min_seq_req):
        for j in range(len(ids)):
            dataset[i][j][0] = pos[i][j][0]
            dataset[i][j][1] = pos[i][j][1]


    #get predictions based on pedestrians positions without robot in path
    #print('Computing predictions...')
    init_predictions = compute_social_predictions(
        model, dataset, args.sample_steps, device, stochastic=args.stochastic)

    ground_truths = torch.zeros((rbt_steps, num_agents, 2), dtype=torch.float,
    device=device)

    #addding the ground truths into dataset to fit prediction framework
    for j in range(len(ids)):
        ground_truths[0][j][0] = pos[-1][j][0]
        ground_truths[0][j][1] = pos[-1][j][1]
        dataset_rbt[0][j][0] = pos[-1][j][0]
        dataset_rbt[0][j][1] = pos[-1][j][1]

    for j in range(len(ids)):
        #for i in range(rbt_steps):
        for i in range(3):
            dataset_rbt[i][j][0] =\
            init_predictions[0]['position'][0][j][0][i][0]
            dataset_rbt[i][j][1] =\
            init_predictions[0]['position'][0][j][0][i][1]
        for i in range(rbt_steps):
            ground_truths[i][j][0] =\
            init_predictions[0]['position'][0][j][0][i][0]
            ground_truths[i][j][1] =\
            init_predictions[0]['position'][0][j][0][i][1]

    #convert pedestrain positions to obsticle list for rrt, take last pos a
    #as most valid
    obsticlelist = pos[-1]

    #list for storing path options for rbt
    rbt_paths = []
    results = []
    #getting a path using rrt
    for p in range(num_paths):              #whats going on here TODO
        print('generating a path')
        rbt_path = rbt.GeneratePath(obsticlelist)
        rbt_path_for_predic = rbt_path
        if rbt_path is None:
            print('ERROR, no path found, try again')
            return
        #check if using rrt with theta value
        if len(rbt_path[0]) == 3:
            #remove theta value as not needed for APG-LSTM
            for i in rbt_path_for_predic:
                i.pop()
        rbt_paths.append(rbt_path_for_predic)
        #add rbt path to data set only add first rbt_steps
        r_seq = 0
        for l in range(rbt_steps):
            dataset_rbt[r_seq][num_agents][0] =\
            rbt_path_for_predic[l][0]
            dataset_rbt[r_seq][num_agents][1] =\
            rbt_path_for_predic[l][1]
            r_seq +=1

        print('Computing predictions...')
        predictions = compute_social_predictions(
            model, dataset_rbt, args.sample_steps, device, stochastic=args.stochastic)
        results.append(predictions)

    ped_path = torch.zeros((rbt_steps, num_agents, 2), dtype=torch.float,
    device=device)

    final_ped= torch.zeros((rbt_steps, num_agents, 2), dtype=torch.float,
    device=device)
    cost_cmp = 10000
    final_path = -1
    for p in range(num_paths):
        #print(p)
        for s in range(rbt_steps):
            for a in range(num_agents):
                    if s > 10:
                        ped_path[s][a][0] = results[p][0]['position'][0][a][0][s][0]
                        ped_path[s][a][1] = results[p][0]['position'][0][a][0][s][1]
                    else:
                        ped_path[s][a][0] = results[p][0]['true_position'][s][a][0]
                        ped_path[s][a][1] = results[p][0]['true_position'][s][a][1]

        cost = evaluate_path(ground_truths, ped_path, rbt_paths[p], rbt)
        print('cost for path ',p,cost)
        robot_plots.plot_path_with_predictions(ground_truths, ped_path, rbt_paths[p])
        if cost < cost_cmp:
            cost_cmp = cost
            final_path = p
            final_ped = ped_path

    #end timing
    time_elapsed = (time.process_time() - time_start)
    print('computation time for ', num_paths, 'paths: ', time_elapsed)
    plt = robot_plots.plot_chosen_path(final_ped, rbt_paths[final_path])
    #plt.show()

    pause_sim(False)

    # Save predictions to file
    with open(args.output, 'wb+') as f:
        torch.save((predictions, config), f)

def evaluate_path(ground_truths, predictions, rbt_path, robot):
    percent= PercentToTarget(rbt_path[0], rbt_path[rbt_steps-2],\
    robot.GetGoal())

    #compare to max allowable work per ped
    disturb_cmp = max_work_per_ped * num_agents
    disturb = disturbance(ground_truths, predictions)
    disturb /= disturb_cmp
    return percent*p_weight + disturb*d_weight

def PercentToTarget(pos_start, pos_fin, goal):
    d1 = calcDistance(pos_start, goal)
    d2 = calcDistance(pos_fin, goal)
    return d2/d1

def disturbance(ground, predict):
    #convert from tensor x, y to array polar r, theta
    polar_ground = empty([num_agents, rbt_steps-2, 2])
    polar_base= empty([num_agents, rbt_steps-2, 2])
    polar_predict = empty([num_agents, rbt_steps-2, 2])
    for a in range(num_agents):
        for s in range(rbt_steps-2):
            polar_ground[a][s] = calc_distance_and_angle(ground[s][a],
            ground[s+1][a])

            polar_predict[a][s] = calc_distance_and_angle(predict[s][a],
            predict[s+1][a])

    disturb = 0
    disturb_base = 0
    for ag in range(num_agents):
        for i in range(rbt_steps-2):
            disturb += abs(polar_ground[ag][i][1] -\
            polar_predict[ag][i][1]) + \
            abs((polar_ground[ag][i][0]**2 - polar_predict[ag][i][0]**2)/2)

    return disturb

def calcDistance(from_pos, to_pos):
    dx = to_pos[0] - from_pos[0]
    dy = to_pos[1] - from_pos[1]
    d = math.hypot(dx, dy)
    return d

def calc_distance_and_angle(from_pos, to_pos):
    dx = to_pos[0] - from_pos[0]
    dy = to_pos[1] - from_pos[1]
    d = math.hypot(dx, dy)
    theta = math.atan2(dy, dx)
    return d, theta

def pause_sim(cmd):
    if cmd:
        print('pausing sim')
       # rospy.wait_for_service('pedsim_simulator/pause_simulation')
        try:
            pause = rospy.ServiceProxy('pedsim_simulator/pause_simulation',
            Empty)
            pause()
        except rospy.ServiceException as e:
            print("service call failed: %s"%e)

    else:
        print('unpausing sim')
       # rospy.wait_for_service('pedsim_simulator/unpause_simulation')
        try:
            unpause = rospy.ServiceProxy('pedsim_simulator/unpause_simulation',
            Empty)
            unpause()
        except rospy.ServiceException as e:
            print("service call failed: %s"%e)

def get_ped_positions(ids):
    print("asking for positions")
    rospy.wait_for_service('get_all_tracked_persons')
    positions = empty([len(ids), 2])
    try:
        getStates = rospy.ServiceProxy('get_all_tracked_persons',
        GetAgentPos)
        states = getStates(ids)
        # convert from geometry_msgs Point (x, y, z) to array(i{x,y})
        for i in ids:
            positions[i][0] = states.persons[i].x
            positions[i][1] = states.persons[i].y
        return positions
    except rospy.ServiceException as e:
        print ("Service call failed: %s"%e)

@torch.no_grad()
def compute_social_predictions(model, dataset, sample_steps, device,
                               stochastic=False):
    model.eval()
    assert sample_steps >= 0

    # Create list to store output for each sequence
    results = []
    # Iterate through the sequences
    # Extract dims of the position sequence for readability
    pos_seq = dataset
    seq_len, num_people, dims = pos_seq.shape
    # Create an empty dict to store results of this sequence
    results.append({})
    # Create a hidden state per person if applicable
    try:
        hidden = model.init_hidden(num_people, device)
    except AttributeError as e:
        print('init_hidden not implemented. Assuming non-recursive model')
        hidden = None

    # Store input sequence
    results[-1]['true_position'] =\
        pos_seq[0:seq_len]

    # Create empty dict to accumulate outputs over the sequence
    outputs_this_sequence = {}

    output_length = rbt_steps - model.before_length
    valid_frames = torch.zeros((output_length,), dtype=bool,
                               device=pos_seq.device)
    output_frame = 0

    # Loop through each frame of the sequence (skip first frame)
    for i in range(2, seq_len):
        hidden = model.init_hidden(num_people, device)

        valid_persons = torch.isfinite(
            pos_seq[:]
        ).all(-1).all(0)

        if valid_persons.sum() > 0:
            valid_frames[output_frame] = True

            model_input = model.prepare_input(pos_seq[:, valid_persons, :],
                                              start=i, stop=i+1)
            model.normalise_input(model_input)

            if hidden is None:
                #print('hidden none')
                model_output = model(model_input)
            else:
                # Extract the valid slices of 'hidden'
                valid_hidden = tuple(h[:, valid_persons, :] for h in hidden)
                #valid_hidden = tuple(hidden)
                model_output, valid_hidden = model(
                    model_input, valid_hidden)
                # Update the valid slices of 'hidden'
                for h, v in zip(hidden, valid_hidden):
                    h[:, valid_persons, :] = v

            model_output.update(model.sample_output(
                model_output, stochastic))
            model.unnormalise_target(model_output)
            model_input = model.propagate_input(model_input, model_output)
            model_output.update(model_input)

            # Start a list per item in model_output
            outputs_this_step = {}
            for k, v in model_output.items():
                outputs_this_step[k] = [v]

            for j in range(sample_steps):
                if hidden is None:
                    model_output = model(model_input)
                else:
                    model_output, valid_hidden = model(
                        model_input, valid_hidden)

                model_output.update(model.sample_output(
                    model_output, stochastic))
                model_input = model.propagate_input(
                    model_input, model_output)
                model_output.update(model_input)

                # Add to the list for each item
                for k, v in model_output.items():
                    # Dims of each element are (1, person, feature)
                    outputs_this_step[k].append(v)

            for k, v in outputs_this_step.items():
                # Dims of one step (1, person_valid, pred_frame, feature)
                one_step = torch.stack(v, dim=2)
                # Dims of one_step_full (1, person_all, pred_frame, feature)
                one_step_full = one_step.new_full(
                    (1, num_people) + one_step.shape[2:],
                    float('nan'))
                one_step_full[0, valid_persons, ...] = one_step
                if k in outputs_this_sequence:
                    outputs_this_sequence[k].append(one_step_full)
                else:
                    outputs_this_sequence[k] = [one_step_full]
                #updating pedestrian positions based on predictions
                #ignoring rbt prediction and using truth
                if k == 'position' and i < seq_len -1:
                    for s in range(num_agents):
                        pos_seq[i+1][s] =\
                        one_step_full[0][s][0][0] #or 1??
                       # for it in range(i, seq_len-1):
                       #     pos_seq[it+1][s] =\
                       #     one_step_full[0][s][0][it+1]
                    
        output_frame += 1
    #returns to prepare input for next seq

        for k, v in outputs_this_sequence.items():
            # Dims of seq_output items are (frame, person, pred_frame, feature)
            seq_output = torch.cat(v, dim=0)
            seq_output_full = seq_output.new_full(
                (output_length,) + seq_output.shape[1:], float('nan'))
            seq_output_full[valid_frames, ...] = seq_output
            results[-1][k] = seq_output_full

    return results


if __name__ == '__main__':
    main()
