#!/usr/bin/env python

from __future__ import absolute_import, division, print_function

import rospy
import argparse
import json
import torch
import time

import lstm_motion_model.utils
from numpy import empty
from lstm_motion_model.robot import Robot
from lstm_motion_model import models, datasets
from pedsim_srvs.srv import GetAgentPos
from geometry_msgs.msg import *
from std_srvs.srv import Empty 

#TODO make these part of function, not global
global num_agents 
global num_paths 
global min_seq_req
global rbt_steps 
num_agents = 49
num_paths = 1
min_seq_req = 3
rbt_steps = 10

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--weights',
                        help='Load model weights from pickle file')
    parser.add_argument('-c', '--config',
                        help='Config json file use to instantiate the model',
                        default='config.json')
   # parser.add_argument('-i', '--input', nargs='+',
   #                     help='Run inference on csv dataset(s) provided',
   #                     required=True)
    parser.add_argument('-s', '--sample-steps',
                        help='Number of steps to recursively sample',
                        default=0, type=int)
    parser.add_argument('-m', '--mode', type=str, default=None,
                        help='Plot data with model in given mode')
    parser.add_argument('-o', '--output', help='Save predictions to pkl file',
                        default='results.pkl')
    parser.add_argument('-t', '--stochastic',
                        help='When forecasting with a probabilistic model, '
                        'sample the distribution',
                        action='store_true')
    parser.add_argument('-a', '--augment',
                        help='Apply random augentation to test data',
                        action='store_true')
    parser.add_argument('--no-cuda', action='store_true', help='Disable CUDA')
    parser.add_argument('--device', type=int, default=0, help='Choose device')
    args = parser.parse_args()

    #initialise Ros node
    rospy.init_node("inference")
    rospy.loginfo("inference running")

    # TODO Fix device management to pick by integer
    use_cuda = (not args.no_cuda) and torch.cuda.is_available()
    device = torch.device('cuda:{}'.format(args.device) if use_cuda else 'cpu')

    # Load model config from file and use it to create the model
    print('Loading model config from {}...'.format(args.config))
    with open(args.config, 'r') as f:
        config = json.load(f)
        print('Creating model from config...')
        model = models.instantiate_model(config['model_type'],
                                         config['model_args'], device)


    # Load model weights from file
    if args.weights:
        print('Loading model weights from {}...'.format(args.weights))
        model.load_state_dict(torch.load(args.weights, map_location=device))
    else:
        print('### WARNING ###')
        print('No weights loaded, fine for a simple model but not for a '
              'learned model')
    model.eval()

    ## Compose dataset transform functions
    #def data_transform(seq):
    #    return lstm_motion_model.utils.random_rotate_tensor(
    #        lstm_motion_model.utils.random_flip_tensor(seq))

    #if args.augment:
    #    tf = data_transform
    #else:
    #    tf = None

    #print('Loading input trajectories from', args.input)
    #dataset = datasets.CrowdSequenceDataset(
    #    args.input,
    #    device=device,
    #    transform=tf)

    ##need to pass in the time sequence to generate path for
    #create robot with its pos and goal and speed
    #hardcoded for now
    robot_pos = [-13, 1]
    robot_goal_pos = [10,-2]
    #will need to change this calc
    #rbt.SetSpeed(pos_seq)
    inflate = 1     #radius of pedestrians as obsticles
    rbt = Robot(robot_pos, robot_goal_pos)

    #get data from sim to create dataset
    dataset = torch.zeros((num_paths, rbt_steps, num_agents+2, 2), dtype=torch.float,
    device=device)

    ids = []
    pos = []
    for i in range(num_agents):
        ids.append(i)
    print(len(ids))

    #generates the min number of recorded position for APG
    for i in range(min_seq_req):
        pos.append(get_ped_positions(ids))
        time.sleep(0.2)

    pause_sim(True)

    #add pedestrian data into dataset, atm just one set
    for p in range(num_paths):
        for i in range(min_seq_req):
            for j in range(len(ids)):
                dataset[p][i][j][0] = pos[i][j][0]
                dataset[p][i][j][1] = pos[i][j][1]

    #convert pedestran positions to obsticle list for rrt, take last pos a
    #as most valid
    obsticlelist = pos[-1]

    #getting a path using rrt
    for p in range(num_paths):
        rbt_path = rbt.GeneratePath(obsticlelist)
        rbt_path_for_predic = rbt_path
        if rbt_path is None:
            #try again  TODO check this works in python
            p= p-1
            continue
        #check if using rrt with theta value
        if len(rbt_path[0]) == 3:
            #remove theta value as not needed for APG-LSTM
            for i in rbt_path_for_predic:
                i.pop()
            print('\npath after removing theta', rbt_path_for_predic)
        #add rbt path to data set only add first rbt_steps
        r_seq = 0
        for (x,y) in rbt_path_for_predic:
            dataset[p][r_seq][num_agents+1][0] = x
            dataset[p][r_seq][num_agents+1][1] = y
            r_seq +=1
            if r_seq >= rbt_steps:
                break


    print(dataset.shape)

    if args.mode:
        try:
            model.mode(args.mode)
            print('Set mode to {}'.format(args.mode))
        except AttributeError:
            print('This model does not support mode changes')

    print('Computing predictions...')
    predictions = compute_social_predictions(
        model, dataset, args.sample_steps, device, stochastic=args.stochastic)

    pause_sim(False)
    #print(pos[2])

    #convert to dataset type - torch tensor dataset([num_datasets][ped_seq])
    #rbt_path_tensor = torch.zeros((seq_len, 1, 2), dtype=torch.float,
                                   #device=pos_seq.device)


    #dataset = torch.tensor(pos,dtype=torch.float,
    #device=device)
    #print(len(predictions))
    #print(len(predictions[0]))
    ##dims are getting mixed somewhere, seem to be predicting 49 steps in
    ##future?
    #print(len(predictions[0]['position'][0]))
    #print(len(predictions[-1]))
    print(predictions[0]['position'][0][0])
    print(pos[0][0])
    print(pos[1][0])
    print(pos[2][0])
    print(len(predictions[0]['position']))

    # Save predictions to file
    with open(args.output, 'wb+') as f:
        torch.save((predictions, config), f)

def pause_sim(cmd):
    if cmd:
        print('pausing sim')
        rospy.wait_for_service('pedsim_simulator/pause_simulation')
        try:
            pause = rospy.ServiceProxy('pedsim_simulator/pause_simulation',
            Empty)
            pause()
        except rospy.ServiceException as e:
            print("service call failed: %s"%e)

    else:
        print('unpausing sim')
        rospy.wait_for_service('pedsim_simulator/unpause_simulation')
        try:
            unpause = rospy.ServiceProxy('pedsim_simulator/unpause_simulation',
            Empty)
            unpause()
        except rospy.ServiceException as e:
            print("service call failed: %s"%e)

def get_ped_positions(ids):
    print("asking for positions")
    rospy.wait_for_service('get_all_tracked_persons')
    positions = empty([len(ids), 2])
    try:
        getStates = rospy.ServiceProxy('get_all_tracked_persons',
        GetAgentPos)
        states = getStates(ids)
        # convert from geometry_msgs Point (x, y, z) to array(i{x,y})
        for i in ids:
            positions[i][0] = states.persons[i].x
            positions[i][1] = states.persons[i].y
        return positions
    except rospy.ServiceException as e:
        print ("Service call failed: %s"%e)

@torch.no_grad()
def compute_social_predictions(model, dataset, sample_steps, device,
                               stochastic=False):
    model.eval()
    assert sample_steps >= 0

    # Create list to store output for each sequence
    results = []
    # Iterate through the sequences
    for pos_seq in dataset:
        #print(pos_seq[0])
        #print(pos_seq[1])
        #print(pos_seq[2])
        #print("raw data shape ", pos_seq.shape)
        #pass data to robot to generate paths
        #robot_pos = [0,0]
        # Extract dims of the position sequence for readability
        seq_len, num_people, dims = pos_seq.shape

        #robot_pos = [int(pos_seq[1,1,0]+1), int(pos_seq[1,1,1]+2)]
        #robot_goal_pos = [int(pos_seq[500,1,0]+1), int(pos_seq[500,1,1]+2)]
        #rbt = Robot(robot_pos, robot_goal_pos)
        #rbt.SetSpeed(pos_seq)
        ##need to pass in the time sequence to generate path for
        #rbt_path = rbt.GeneratePath(pos_seq[1,:,:])
        ##rbt_path_tensor = torch.Tensor(list(rbt_path))
        #rbt_path_tensor = torch.zeros((seq_len, 1, 2), dtype=torch.float,
                                       #device=pos_seq.device)
        #repeats robots path
        #c = 0
        #for i in range(seq_len):
        #    if i >= len(rbt_path):
        #        c = 0
        #    rbt_path_tensor[i][0][0] = rbt_path[c][0]
        #    rbt_path_tensor[i][0][1] = rbt_path[c][1]
        #    c += 1
        #print(pos_seq[:,-1,:])
        #add robots path into nueral net predictions
        #pos_seq = torch.cat([pos_seq, rbt_path_tensor],1)
        #update Robots idx in ped_seq
        #rbt.UpdateRobotIdx(pos_shape[1])


        #print(pos_seq[:,-1,:])
        #print("pos_shape ", pos_shape)

        # Create an empty dict to store results of this sequence
        results.append({})
        # Create a hidden state per person if applicable
        try:
            hidden = model.init_hidden(num_people, device)
        except AttributeError as e:
            print(e)
            print('init_hidden not implemented. Assuming non-recursive model')
            hidden = None

        # Store input sequence
        results[-1]['true_position'] =\
            pos_seq[0:seq_len]
            #pos_seq[model.before_length:len(pos_seq)-model.after_length]

        # Create empty dict to accumulate outputs over the sequence
        outputs_this_sequence = {}

        #output_length = seq_len - model.before_length - model.after_length
        output_length = min_seq_req 
        print("output length", output_length)
        valid_frames = torch.zeros((output_length,), dtype=bool,
                                   device=pos_seq.device)
        output_frame = 0
        print("model.before_length", model.before_length)
        print("model.after_length", model.after_length)
        print("range", seq_len - model.after_length)

        # Loop through each frame of the sequence (skip first frame)
        #for i in range(model.before_length, seq_len-model.after_length):
        for i in range(2, seq_len):

            # Valid people are those observed in this frame and the one before
           # valid_persons = torch.isfinite(
           #     pos_seq[i - model.before_length:i + model.after_length + 1]
           # ).all(-1).all(0)
            valid_persons = torch.isfinite(
                pos_seq[0:seq_len]
            ).all(-1).all(0)

            #print("valid persons", valid_persons)

            if valid_persons.sum() > 0:
                valid_frames[output_frame] = True
                print("valid_persons sum", valid_persons.sum())

                #model_input = model.prepare_input(pos_seq[:, valid_persons, :],
                #                                  start=i, stop=i+1)
                model_input = model.prepare_input(pos_seq[:, :, :],
                                                  start=i, stop=i+1)
                model.normalise_input(model_input)

                if hidden is None:
                    model_output = model(model_input)
                else:
                    # Extract the valid slices of 'hidden'
                    valid_hidden = tuple(h[:, :, :] for h in hidden)
                    #valid_hidden = tuple(hidden)
                    model_output, valid_hidden = model(
                        model_input, valid_hidden)
                    # Update the valid slices of 'hidden'
                    for h, v in zip(hidden, valid_hidden):
                        h[:, :, :] = v

                model_output.update(model.sample_output(
                    model_output, stochastic))
                model.unnormalise_target(model_output)
                model_input = model.propagate_input(model_input, model_output)
                model_output.update(model_input)

                # Start a list per item in model_output
                outputs_this_step = {}
                for k, v in model_output.items():
                    outputs_this_step[k] = [v]

                for j in range(sample_steps):
                    if hidden is None:
                        model_output = model(model_input)
                    else:
                        model_output, valid_hidden = model(
                            model_input, valid_hidden)

                    model_output.update(model.sample_output(
                        model_output, stochastic))
                    model_input = model.propagate_input(
                        model_input, model_output)
                    model_output.update(model_input)

                    # Add to the list for each item
                    for k, v in model_output.items():
                        # Dims of each element are (1, person, feature)
                        outputs_this_step[k].append(v)

                for k, v in outputs_this_step.items():
                    # Dims of one step (1, person_valid, pred_frame, feature)
                    one_step = torch.stack(v, dim=2)
                    # Dims of one_step_full (1, person_all, pred_frame, feature)
                    one_step_full = one_step.new_full(
                        (1, num_people) + one_step.shape[2:],
                        float('nan'))
                    one_step_full[0, valid_persons, ...] = one_step
                    if k in outputs_this_sequence:
                        outputs_this_sequence[k].append(one_step_full)
                    else:
                        outputs_this_sequence[k] = [one_step_full]

            output_frame += 1

        for k, v in outputs_this_sequence.items():
            # Dims of seq_output items are (frame, person, pred_frame, feature)
            seq_output = torch.cat(v, dim=0)
            seq_output_full = seq_output.new_full(
                (output_length,) + seq_output.shape[1:], float('nan'))
            seq_output_full[valid_frames, ...] = seq_output
            results[-1][k] = seq_output_full

    return results


if __name__ == '__main__':
    main()
